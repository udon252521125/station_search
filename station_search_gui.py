import streamlit as st
import pandas as pd
import json
import re
import jaconv
from typing import List, Dict, Tuple, Optional
import io
import os
from master_data import LINE_MAPPING, OPERATOR_MAPPING, REGION_MAPPING, PREFECTURE_CODE_TO_NAME


@st.cache_data
def load_precomputed_index():
    """
    äº‹å‰è¨ˆç®—ã•ã‚ŒãŸã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿
    
    Returns:
        tuple: (hiragana_index, katakana_i                    # æ¤œç´¢ç¯„å›²ã‚’æ±ºå®š
                    station_pref_cd = station_data.get('pref_cd', 0)
                    if selected_prefecture_codes and station_pref_cd in selected_prefecture_codes:
                        search_scope = 'ğŸ”µ é¸æŠåœ°åŸŸå†…'
                    else:
                        search_scope = 'ğŸ”´ å…¨å›½'df) or (None, None, None)
    """
    try:
        # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ã®å­˜åœ¨ç¢ºèª
        hiragana_file = 'station_hiragana_index.json'
        katakana_file = 'station_katakana_index.json'
        data_file = 'station_data_indexed.csv'
        
        if not all(os.path.exists(f) for f in [hiragana_file, katakana_file, data_file]):
            return None, None, None
        
        # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹èª­ã¿è¾¼ã¿
        with open(hiragana_file, 'r', encoding='utf-8') as f:
            hiragana_index = json.load(f)
        
        with open(katakana_file, 'r', encoding='utf-8') as f:
            katakana_index = json.load(f)
        
        # é§…ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
        df = pd.read_csv(data_file)
        
        # ã‚­ãƒ¼ã‚’æ•´æ•°ã«å¤‰æ›ï¼ˆJSONã¯æ–‡å­—åˆ—ã‚­ãƒ¼ã«ãªã‚‹ãŸã‚ï¼‰
        hiragana_index = {int(k): v for k, v in hiragana_index.items()}
        katakana_index = {int(k): v for k, v in katakana_index.items()}
        
        return hiragana_index, katakana_index, df
        
    except Exception as e:
        st.warning(f"ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—: {e}")
        return None, None, None


def find_stations_by_index(station_index: Dict, char: str, position: int, df: pd.DataFrame) -> List[Dict]:
    """
    ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ä½¿ç”¨ã—ãŸé«˜é€Ÿé§…æ¤œç´¢
    
    Args:
        station_index: äº‹å‰ä½œæˆã•ã‚ŒãŸã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
        char: æ¤œç´¢æ–‡å­—
        position: ä½ç½®
        df: é§…ãƒ‡ãƒ¼ã‚¿
    
    Returns:
        è©²å½“ã™ã‚‹é§…ã®è¾æ›¸ãƒªã‚¹ãƒˆ
    """
    if position not in station_index or char not in station_index[position]:
        return []
    
    # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‹ã‚‰é§…IDã‚’å–å¾—ã—ã€å¯¾å¿œã™ã‚‹é§…ãƒ‡ãƒ¼ã‚¿ã‚’è¿”ã™
    station_indices = station_index[position][char]
    matching_stations = []
    
    for idx in station_indices:
        if idx < len(df):
            station_data = df.iloc[idx].to_dict()
            
            # å®Ÿéš›ã®é§…åã‹ã‚‰è©²å½“ä½ç½®ã®æ–‡å­—ã‚’å–å¾—ã—ã¦æ¤œè¨¼
            station_name = station_data.get('station_name', '')
            if position < len(station_name):
                # å®Ÿéš›ã®é§…åã®æŒ‡å®šä½ç½®ã®æ–‡å­—ã‚’å–å¾—
                actual_char_at_position = station_name[position]
                
                # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãŒæ­£ã—ãå‹•ä½œã—ã¦ã„ã‚‹ã‹ç¢ºèªï¼ˆãƒ‡ãƒãƒƒã‚°ç”¨ï¼‰
                # ã“ã®æ–‡å­—ãŒæ¤œç´¢æ–‡å­—ã¨ä¸€è‡´ã™ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
                normalized_actual = jaconv.kata2hira(actual_char_at_position)
                normalized_search = jaconv.kata2hira(char)
                
                if normalized_actual == normalized_search:
                    station_data['actual_char'] = actual_char_at_position
                    matching_stations.append(station_data)
            # position >= len(station_name)ã®å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—ï¼ˆã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚¨ãƒ©ãƒ¼ã®å¯èƒ½æ€§ï¼‰
    
    return matching_stations



def get_prefecture_options():
    """éƒ½é“åºœçœŒã¨ãã®åœ°æ–¹åŒºåˆ†ã®ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã‚’å–å¾—ï¼ˆéƒ½é“åºœçœŒã‚³ãƒ¼ãƒ‰é †ï¼‰"""
    options = []
    
    # åœ°æ–¹åŒºåˆ†ã‚’ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã«è¿½åŠ 
    for region_name in REGION_MAPPING.keys():
        options.append(f"ã€{region_name}ã€‘")
    
    # éƒ½é“åºœçœŒã‚’éƒ½é“åºœçœŒã‚³ãƒ¼ãƒ‰é †ã§è¿½åŠ 
    for code in sorted(PREFECTURE_CODE_TO_NAME.keys()):
        options.append(PREFECTURE_CODE_TO_NAME[code])
    
    return options

def get_selected_prefecture_codes(selected_options):
    """é¸æŠã•ã‚ŒãŸã‚ªãƒ—ã‚·ãƒ§ãƒ³ã‹ã‚‰éƒ½é“åºœçœŒã‚³ãƒ¼ãƒ‰ã®ãƒªã‚¹ãƒˆã‚’å–å¾—"""
    pref_codes = []
    
    for option in selected_options:
        if option.startswith("ã€") and option.endswith("ã€‘"):
            # åœ°æ–¹åŒºåˆ†ã®å ´åˆ
            region_name = option[1:-1]  # ã€ã€‘ã‚’é™¤å»
            if region_name in REGION_MAPPING:
                pref_codes.extend(REGION_MAPPING[region_name])
        else:
            # å€‹åˆ¥éƒ½é“åºœçœŒã®å ´åˆ
            for code, name in PREFECTURE_CODE_TO_NAME.items():
                if name == option:
                    pref_codes.append(code)
                    break
    
    return list(set(pref_codes))  # é‡è¤‡æ’é™¤


def load_station_data(csv_file=None) -> pd.DataFrame:
    """
    é§…ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ã€å¿…è¦ãªåˆ—ã®å­˜åœ¨ç¢ºèªã¨ãƒ‡ãƒ¼ã‚¿æ•´å½¢ã‚’è¡Œã†
    """
    try:
        if csv_file is not None:
            df = pd.read_csv(csv_file)
        else:
            df = pd.read_csv("station20250604free.csv")
        
        # å¿…è¦ãªåˆ—ã®å­˜åœ¨ç¢ºèª
        required_columns = ['station_name', 'pref_cd']
        missing_columns = [col for col in required_columns if col not in df.columns]
        
        if missing_columns:
            st.error(f"éƒ½é“åºœçœŒåˆ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {', '.join(missing_columns)}")
            return pd.DataFrame()
        
        # éƒ½é“åºœçœŒåã‚’è¿½åŠ 
        df['prefecture'] = df['pref_cd'].map(PREFECTURE_CODE_TO_NAME)
        
        # è·¯ç·šæƒ…å ±ã‚’èª­ã¿è¾¼ã¿
        try:
            route_df = pd.read_csv("eki.csv")
            route_mapping = dict(zip(route_df['è·¯ç·šã‚³ãƒ¼ãƒ‰'], route_df['è·¯ç·šå']))
            operator_mapping = dict(zip(route_df['è·¯ç·šã‚³ãƒ¼ãƒ‰'], route_df['äº‹æ¥­è€…']))
            
            # è·¯ç·šåã¨äº‹æ¥­è€…åã‚’ãƒãƒƒãƒ”ãƒ³ã‚°
            df['route_name'] = df['line_cd'].map(route_mapping).fillna("ä¸æ˜")
            df['operator_name'] = df['line_cd'].map(operator_mapping).fillna("ä¸æ˜")
        except Exception as e:
            st.warning(f"è·¯ç·šãƒ‡ãƒ¼ã‚¿(eki.csv)ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
            df['route_name'] = "ä¸æ˜"
            df['operator_name'] = "ä¸æ˜"
        
        return df
        
    except Exception as e:
        st.error(f"CSVãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
        return pd.DataFrame()


def normalize_search_string(text: str, include_katakana: bool = False) -> str:
    """
    æ¤œç´¢æ–‡å­—åˆ—ã‚’æ­£è¦åŒ–ï¼ˆã²ã‚‰ãŒãªãƒ»æ¼¢å­—å¯¾å¿œã€æœ€å¤§20æ–‡å­—ï¼‰
    
    Args:
        text: æ¤œç´¢æ–‡å­—åˆ—
        include_katakana: Trueã®å ´åˆã‚«ã‚¿ã‚«ãƒŠã‚‚ä¿æŒã€Falseã®å ´åˆã²ã‚‰ãŒãªã«å¤‰æ›
    """
    if not text:
        return ""
    
    if include_katakana:
        # ã‚«ã‚¿ã‚«ãƒŠã‚‚ä¿æŒã™ã‚‹å ´åˆï¼šã²ã‚‰ãŒãªã€ã‚«ã‚¿ã‚«ãƒŠã€æ¼¢å­—ã‚’ä¿æŒ
        text = re.sub(r'[^ã-ã‚“ã‚¡-ãƒ¾ãƒ¼ä¸€-é¾¯]', '', text)
    else:
        # ã‚«ã‚¿ã‚«ãƒŠã‚’ã²ã‚‰ãŒãªã«å¤‰æ›
        text = jaconv.kata2hira(text)
        # ã²ã‚‰ãŒãªã¨æ¼¢å­—ã®ã¿ã‚’ä¿æŒ
        text = re.sub(r'[^ã-ã‚“ãƒ¼ä¸€-é¾¯]', '', text)
    
    # æœ€å¤§20æ–‡å­—ã«åˆ¶é™
    return text[:20]


def find_character_positions_cross(df: pd.DataFrame, search_string: str, include_katakana: bool = False) -> Dict:
    """
    è¤‡æ•°é§…åã‚’ä½¿ã£ãŸç¸¦ã‚¯ãƒ­ã‚¹ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢ã®ãŸã‚ã€å„æ–‡å­—ã®ä½ç½®ä¸€è‡´ã‚’åˆ¤å®š
    """
    if not search_string:
        return {"cross_possible": False, "position_groups": {}, "matching_stations": []}
    
    # æ¤œç´¢æ–‡å­—åˆ—ã®å„æ–‡å­—ã«ã¤ã„ã¦ã€ãã®æ–‡å­—ã‚’å«ã‚€é§…ã‚’ä½ç½®åˆ¥ã«åˆ†é¡
    char_positions = {}
    
    for char_index, char in enumerate(search_string):
        char_positions[char_index] = {}  # ä½ç½®åˆ¥ã®é§…ãƒªã‚¹ãƒˆ
        
        for _, row in df.iterrows():
            station_name = row['station_name']
            
            # æ¤œç´¢å¯¾è±¡ã®æ±ºå®š
            if include_katakana:
                # ã‚«ã‚¿ã‚«ãƒŠã‚‚ä¿æŒã™ã‚‹å ´åˆï¼šæ–‡å­—å¤‰æ›ã‚’è¡Œã‚ãªã„
                search_target = station_name
            else:
                # ã‚«ã‚¿ã‚«ãƒŠã‚’ã²ã‚‰ãŒãªã«å¤‰æ›ã™ã‚‹å ´åˆ
                if re.match(r'[ã-ã‚“ãƒ¼]', char):
                    # ã²ã‚‰ãŒãªæ¤œç´¢
                    search_target = jaconv.kata2hira(station_name)
                else:
                    # æ¼¢å­—æ¤œç´¢
                    search_target = station_name
            
            # æ–‡å­—ã®ä½ç½®ã‚’ã™ã¹ã¦å–å¾—
            for pos in range(len(search_target)):
                if pos < len(search_target) and search_target[pos] == char:
                    if pos not in char_positions[char_index]:
                        char_positions[char_index][pos] = []
                    char_positions[char_index][pos].append(row.to_dict())
    
    # å…¨æ–‡å­—ãŒåŒã˜ä½ç½®ã§è¦‹ã¤ã‹ã‚‹çµ„ã¿åˆã‚ã›ã‚’æ¢ã™
    cross_possible = False
    best_position = None
    matching_stations = []
    
    # å„ä½ç½®ã«ã¤ã„ã¦ã€å…¨æ–‡å­—ãŒæƒã†ã‹ãƒã‚§ãƒƒã‚¯
    for pos in range(20):  # æœ€å¤§20æ–‡å­—ã®é§…åã‚’æƒ³å®š
        station_set = []
        valid_position = True
        
        for char_index in range(len(search_string)):
            if pos in char_positions[char_index] and len(char_positions[char_index][pos]) > 0:
                # ã“ã®ä½ç½®ã«ã“ã®æ–‡å­—ã‚’æŒã¤é§…ãŒã‚ã‚‹
                station_set.append(char_positions[char_index][pos])
            else:
                # ã“ã®ä½ç½®ã«ã“ã®æ–‡å­—ã‚’æŒã¤é§…ãŒãªã„
                valid_position = False
                break
        
        if valid_position and len(station_set) == len(search_string):
            cross_possible = True
            best_position = pos
            # å„æ–‡å­—ã«ã¤ã„ã¦1ã¤ãšã¤é§…ã‚’é¸æŠï¼ˆãƒ©ãƒ³ãƒ€ãƒ ã«é¸ã¶ã®ã§ã¯ãªãæœ€åˆã®é§…ï¼‰
            for char_stations in station_set:
                if char_stations:
                    matching_stations.append(char_stations[0])
            break
    
    return {
        "cross_possible": cross_possible,
        "position": best_position,
        "matching_stations": matching_stations,
        "position_groups": char_positions
    }


def find_character_positions_cross_with_priority(df_all: pd.DataFrame, df_selected: pd.DataFrame, search_string: str, include_katakana: bool = False) -> Dict:
    """
    æ–‡å­—åˆ¥å„ªå…ˆæ¤œç´¢ã‚’è¡Œã†ç¸¦ã‚¯ãƒ­ã‚¹ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢é–¢æ•°
    å„æ–‡å­—ã«ã¤ã„ã¦ã€é¸æŠåœ°åŸŸå†…ã®é§…ã‚’å„ªå…ˆçš„ã«ä½¿ç”¨ã—ã€ãªã‘ã‚Œã°å…¨å›½ã‹ã‚‰é¸æŠ
    è©²å½“ã™ã‚‹å…¨ã¦ã®é§…ã‚’è¿”ã™ï¼ˆå…¨ã¦ã®ä½ç½®ã§ã®çµ„ã¿åˆã‚ã›ï¼‰
    """
    if not search_string:
        return {"cross_possible": False, "position_groups": {}, "matching_stations": []}
    
    all_position_results = []
    
    # å„ä½ç½®ã«ã¤ã„ã¦ã€å…¨æ–‡å­—ãŒæƒã†ã‹ãƒã‚§ãƒƒã‚¯ï¼ˆå…¨ä½ç½®ã‚’èª¿ã¹ã‚‹ï¼‰
    for pos in range(20):  # æœ€å¤§20æ–‡å­—ã®é§…åã‚’æƒ³å®š
        all_matching_stations = []
        all_chars_found = True
        
        for char_index, char in enumerate(search_string):
            char_stations = []
            
            # ã¾ãšé¸æŠåœ°åŸŸå†…ã§æ¢ã™ï¼ˆå…¨ã¦ã®è©²å½“é§…ï¼‰
            if not df_selected.empty:
                selected_stations = find_all_chars_at_position(df_selected, char, pos, include_katakana)
                char_stations.extend(selected_stations)
            
            # é¸æŠåœ°åŸŸå¤–ã‚‚å«ã‚ã¦å…¨å›½ã§æ¢ã™ï¼ˆé‡è¤‡ã¯å¾Œã§é™¤å»ï¼‰
            all_stations = find_all_chars_at_position(df_all, char, pos, include_katakana)
            
            # é‡è¤‡ã‚’é™¤å»ã—ã¤ã¤è¿½åŠ ï¼ˆstation_nameã¨pref_cdã§åˆ¤å®šï¼‰
            existing_keys = {(s['station_name'], s.get('pref_cd', 0)) for s in char_stations}
            for station in all_stations:
                key = (station['station_name'], station.get('pref_cd', 0))
                if key not in existing_keys:
                    char_stations.append(station)
            
            if char_stations:
                all_matching_stations.append((char, char_stations))
            else:
                all_chars_found = False
                break
        
        # ã“ã®ä½ç½®ã§å…¨æ–‡å­—ãŒæƒã£ãŸå ´åˆã¯çµæœã«è¿½åŠ 
        if all_chars_found and len(all_matching_stations) == len(search_string):
            all_position_results.append({
                "position": pos,
                "matching_stations": all_matching_stations
            })
    
    # çµæœãŒã‚ã‚Œã°è¿”ã™
    if all_position_results:
        return {
            "cross_possible": True,
            "all_positions": all_position_results,
            "position_groups": {}
        }
    
    return {"cross_possible": False, "position_groups": {}, "matching_stations": []}


def find_all_chars_at_position(df: pd.DataFrame, char: str, position: int, include_katakana: bool = False) -> List[Dict]:
    """
    æŒ‡å®šã•ã‚ŒãŸä½ç½®ã«æŒ‡å®šã•ã‚ŒãŸæ–‡å­—ã‚’æŒã¤å…¨ã¦ã®é§…ã‚’æ¢ã™
    å¯¾å¿œæ–‡å­—ã¯å…ƒã®é§…åã‹ã‚‰å–å¾—ã™ã‚‹
    """
    matching_stations = []
    
    for _, row in df.iterrows():
        station_name = row['station_name']
        
        # æ¤œç´¢å¯¾è±¡ã®æ±ºå®š
        if include_katakana:
            # ã‚«ã‚¿ã‚«ãƒŠã‚‚ä¿æŒã™ã‚‹å ´åˆï¼šæ–‡å­—å¤‰æ›ã‚’è¡Œã‚ãªã„
            search_target = station_name
            search_char_normalized = char
        else:
            # ã‚«ã‚¿ã‚«ãƒŠã‚’ã²ã‚‰ãŒãªã«å¤‰æ›ã™ã‚‹å ´åˆ
            search_target = jaconv.kata2hira(station_name)
            search_char_normalized = jaconv.kata2hira(char)
        
        # æŒ‡å®šä½ç½®ã«æŒ‡å®šæ–‡å­—ãŒã‚ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
        if position < len(search_target) and search_target[position] == search_char_normalized:
            # çµæœãƒ‡ãƒ¼ã‚¿ã«å…ƒã®é§…åã®æ–‡å­—ã‚’è¿½åŠ 
            station_dict = row.to_dict()
            # å¯¾å¿œæ–‡å­—ã¯å…ƒã®é§…åã‹ã‚‰å–å¾—ï¼ˆã‚«ã‚¿ã‚«ãƒŠãªã‚‰ã‚«ã‚¿ã‚«ãƒŠã®ã¾ã¾ï¼‰
            if position < len(station_name):
                station_dict['actual_char'] = station_name[position]
            else:
                station_dict['actual_char'] = char
            matching_stations.append(station_dict)
    
    return matching_stations


def search_and_analyze_fast(df: pd.DataFrame, search_string: str, selected_prefecture_codes: List[int], hiragana_index: Dict, katakana_index: Dict, include_katakana: bool = False) -> pd.DataFrame:
    """
    ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ä½¿ç”¨ã—ãŸé«˜é€Ÿç¸¦ã‚¯ãƒ­ã‚¹ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢ã¨åˆ†æ
    """
    if df.empty:
        return pd.DataFrame()
    
    # æ¤œç´¢æ–‡å­—åˆ—ã®æ­£è¦åŒ–
    normalized_search = normalize_search_string(search_string, include_katakana)
    
    if not normalized_search:
        return pd.DataFrame()
    
    # ä½¿ç”¨ã™ã‚‹ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’é¸æŠ
    station_index = katakana_index if include_katakana else hiragana_index
    
    result_rows = []
    
    # å„ä½ç½®ã«ã¤ã„ã¦ã€ç¸¦ã‚¯ãƒ­ã‚¹ãƒ¯ãƒ¼ãƒ‰ã®å¯èƒ½æ€§ã‚’ãƒã‚§ãƒƒã‚¯
    for pos in range(20):  # æœ€å¤§20æ–‡å­—ã®é§…åã‚’æƒ³å®š
        # å„æ–‡å­—ã”ã¨ã«è©²å½“ã™ã‚‹é§…ã®ãƒªã‚¹ãƒˆã‚’å–å¾—
        char_station_lists = []
        
        for char_index, char in enumerate(normalized_search):
            # æ¤œç´¢æ–‡å­—ã‚’é©åˆ‡ã«æ­£è¦åŒ–
            if include_katakana:
                search_char = char  # ã‚«ã‚¿ã‚«ãƒŠãƒ¢ãƒ¼ãƒ‰ã§ã¯å¤‰æ›ã—ãªã„
            else:
                search_char = jaconv.kata2hira(char)  # ã²ã‚‰ãŒãªãƒ¢ãƒ¼ãƒ‰ã§ã¯ã²ã‚‰ãŒãªã«å¤‰æ›
            
            # ã“ã®ä½ç½®ã«ã“ã®æ–‡å­—ã‚’æŒã¤é§…ã‚’æ¢ã™
            char_stations = find_stations_by_index(station_index, search_char, pos, df)
            
            # ã“ã®æ–‡å­—ã«å¯¾å¿œã™ã‚‹é§…ãŒãªã„å ´åˆã¯ã€ã“ã®ä½ç½®ã§ã¯ç¸¦ã‚¯ãƒ­ã‚¹ãƒ¯ãƒ¼ãƒ‰ä¸å¯èƒ½
            if not char_stations:
                break
                
            char_station_lists.append(char_stations)
        
        # å…¨ã¦ã®æ–‡å­—ã§é§…ãŒè¦‹ã¤ã‹ã£ãŸå ´åˆã®ã¿çµæœã«è¿½åŠ 
        if len(char_station_lists) == len(normalized_search):
            # å„æ–‡å­—ã”ã¨ã«é§…ã‚’çµæœã«è¿½åŠ 
            for char_index, stations_for_char in enumerate(char_station_lists):
                for station_data in stations_for_char:
                    # æ¤œç´¢ç¯„å›²ã‚’æ±ºå®š
                    station_pref_cd = station_data.get('pref_cd', 0)
                    if selected_prefecture_codes and station_pref_cd in selected_prefecture_codes:
                        search_scope = 'ğŸ”µ é¸æŠåœ°åŸŸå†…'
                    else:
                        search_scope = 'ğŸ”´ å…¨å›½'
                    
                    # å¯¾å¿œæ–‡å­—ã¯å…ƒã®é§…åã‹ã‚‰å®Ÿéš›ã«å–å¾—
                    station_name = station_data.get('station_name', '')
                    if pos < len(station_name):
                        actual_char = station_name[pos]
                    else:
                        actual_char = normalized_search[char_index]
                    
                    result_rows.append({
                        'station_name': station_data['station_name'],
                        'prefecture': station_data.get('prefecture', 'ä¸æ˜'),
                        'operator_name': station_data.get('operator_name', 'ä¸æ˜'),
                        'route_name': station_data.get('route_name', 'ä¸æ˜'),
                        'search_char': actual_char,
                        'char_position': pos + 1,  # 1ãƒ™ãƒ¼ã‚¹ã«å¤‰æ›
                        'search_scope': search_scope
                    })
    
    return pd.DataFrame(result_rows)


def search_and_analyze(df: pd.DataFrame, search_string: str, selected_prefecture_codes: List[int], include_katakana: bool = False) -> pd.DataFrame:
    """
    è¤‡æ•°é§…åã‚’ä½¿ã£ãŸç¸¦ã‚¯ãƒ­ã‚¹ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢ã¨åˆ†æï¼ˆæ–‡å­—åˆ¥å„ªå…ˆé †ä½ä»˜ãï¼‰
    """
    if df.empty:
        return pd.DataFrame()
    
    # æ¤œç´¢æ–‡å­—åˆ—ã®æ­£è¦åŒ–
    normalized_search = normalize_search_string(search_string, include_katakana)
    
    if not normalized_search:
        # æ¤œç´¢æ–‡å­—åˆ—ãŒç©ºã®å ´åˆã¯ç©ºã‚’è¿”ã™
        return pd.DataFrame()
    
    # é¸æŠåœ°åŸŸã®ãƒ‡ãƒ¼ã‚¿ã¨å…¨å›½ãƒ‡ãƒ¼ã‚¿ã‚’æº–å‚™
    if selected_prefecture_codes:
        df_selected = df[df['pref_cd'].isin(selected_prefecture_codes)].copy()
    else:
        df_selected = pd.DataFrame()
    
    # æ–‡å­—åˆ¥å„ªå…ˆæ¤œç´¢ã‚’å®Ÿè¡Œ
    cross_info = find_character_positions_cross_with_priority(df, df_selected, normalized_search, include_katakana)
    
    if cross_info['cross_possible'] and cross_info.get('all_positions'):
        # ãƒãƒƒãƒã—ãŸé§…ã®æƒ…å ±ã‚’æ•´ç†ï¼ˆè¤‡æ•°ä½ç½®ãƒ»è¤‡æ•°é§…ã‚’å±•é–‹ï¼‰
        result_rows = []
        
        for position_result in cross_info['all_positions']:
            position = position_result['position']
            matching_stations = position_result['matching_stations']
            
            for char_index, (char, stations_list) in enumerate(matching_stations):
                for station_data in stations_list:
                    # æ¤œç´¢ç¯„å›²ã‚’æ±ºå®šï¼ˆçµµæ–‡å­—ä»˜ãï¼‰
                    station_pref_cd = station_data.get('pref_cd', 0)
                    if selected_prefecture_codes and station_pref_cd in selected_prefecture_codes:
                        search_scope = 'ğŸ”µ é¸æŠåœ°åŸŸå†…'
                    else:
                        search_scope = 'ğŸ”´ å…¨å›½'
                    
                    # å¯¾å¿œæ–‡å­—ã¯å…ƒã®é§…åã‹ã‚‰å–å¾—ï¼ˆactual_charãŒã‚ã‚Œã°ãã‚Œã‚’ä½¿ç”¨ï¼‰
                    display_char = station_data.get('actual_char', char)
                    
                    result_rows.append({
                        'station_name': station_data['station_name'],
                        'prefecture': station_data['prefecture'],
                        'operator_name': station_data['operator_name'],
                        'route_name': station_data['route_name'],
                        'search_char': display_char,
                        'char_position': position + 1,  # 1ãƒ™ãƒ¼ã‚¹ã«å¤‰æ›
                        'search_scope': search_scope
                    })
        
        return pd.DataFrame(result_rows)
    else:
        return pd.DataFrame()


def style_dataframe(df):
    """
    ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã«é¸æŠåœ°åŸŸã®èƒŒæ™¯è‰²ã‚¹ã‚¿ã‚¤ãƒ«ã¨å¢ƒç•Œç·šã‚¹ã‚¿ã‚¤ãƒ«ã‚’é©ç”¨
    """
    def highlight_selected_region(row):
        if 'ğŸ”µ é¸æŠåœ°åŸŸå†…' in str(row.get('search_scope', '')):
            return ['background-color: #e3f2fd; border: 2px solid #1976d2'] * len(row)  # è–„ã„é’è‰² + æ¿ƒã„é’ã®å¢ƒç•Œç·š
        else:
            return ['background-color: #fff3e0; border: 2px solid #f57c00'] * len(row)  # è–„ã„ã‚ªãƒ¬ãƒ³ã‚¸è‰² + ã‚ªãƒ¬ãƒ³ã‚¸ã®å¢ƒç•Œç·š
    
    styled_df = df.style.apply(highlight_selected_region, axis=1)
    
    # ã‚»ãƒ«é–“ã®å¢ƒç•Œç·šã‚’å¤ªãã™ã‚‹CSS
    styled_df = styled_df.set_table_styles([
        {
            'selector': 'td',
            'props': [
                ('border', '2px solid #ddd'),
                ('border-collapse', 'separate'),
                ('border-spacing', '0'),
                ('padding', '8px')
            ]
        },
        {
            'selector': 'th',
            'props': [
                ('border', '3px solid #333'),
                ('background-color', '#f5f5f5'),
                ('font-weight', 'bold'),
                ('padding', '10px')
            ]
        },
        {
            'selector': 'table',
            'props': [
                ('border-collapse', 'separate'),
                ('border-spacing', '2px')
            ]
        }
    ])
    
    return styled_df


def create_download_csv(df: pd.DataFrame) -> str:
    """
    ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ç”¨ã®CSVæ–‡å­—åˆ—ã‚’ç”Ÿæˆ
    """
    if df.empty:
        return ""
    
    output = io.StringIO()
    df.to_csv(output, index=False, encoding='utf-8')
    return output.getvalue()


def main():
    st.title("é§…åç¸¦ã‚¯ãƒ­ã‚¹ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢ãƒ„ãƒ¼ãƒ«")
    
    # ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã®è¡¨ç¤º
    st.markdown("ğŸ“Š **ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹**: [é§…ãƒ‡ãƒ¼ã‚¿.jpï¼ˆekidata.jpï¼‰](https://ekidata.jp/) ã®ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨")
    
    # å…¨èˆ¬çš„ãªæ³¨æ„æ›¸ã
    st.warning("""
    âš ï¸ **é‡è¦ãªæ³¨æ„äº‹é …**
    - æ¤œç´¢çµæœé€šã‚Šã«é§…åãŒå°å­—ã•ã‚Œã‚‹ã¨ã¯é™ã‚Šã¾ã›ã‚“ã€‚
    - é‰„é“ä¼šç¤¾ã«ã‚ˆã£ã¦ã€å°å­—ã®æ–¹æ³•ãŒç•°ãªã‚‹å ´åˆãŒã‚ã‚Šã¾ã™ã€‚
    """)
    
    # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿è©¦è¡Œ
    hiragana_index, katakana_index, indexed_df = load_precomputed_index()
    use_fast_search = hiragana_index is not None
    
    if use_fast_search:
        st.success("é«˜é€Ÿã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ä½¿ç”¨ã—ã¾ã™")
    else:
        st.info("ğŸ’¡ é«˜é€ŸåŒ–ã®ãŸã‚ create_index.py ã‚’å®Ÿè¡Œã—ã¦ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ä½œæˆã—ã¦ãã ã•ã„")
    
    # ã‚«ã‚¹ã‚¿ãƒ CSS for æ¤œç´¢ç¯„å›²ã®è¦–è¦šçš„åŒºåˆ¥
    st.markdown("""
    <style>
    /* ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ å…¨ä½“ã®ã‚¹ã‚¿ã‚¤ãƒªãƒ³ã‚° */
    .stDataFrame {
        font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
    }
    
    /* ãƒ†ãƒ¼ãƒ–ãƒ«ã®ãƒœãƒ¼ãƒ€ãƒ¼ã‚’å¼·èª¿ */
    .stDataFrame table {
        border-collapse: separate !important;
        border-spacing: 2px !important;
        border: 3px solid #333 !important;
    }
    
    /* ã‚»ãƒ«ã®ãƒœãƒ¼ãƒ€ãƒ¼ã‚’å¤ªã */
    .stDataFrame td, .stDataFrame th {
        border: 2px solid #666 !important;
        padding: 8px !important;
    }
    
    /* ãƒ˜ãƒƒãƒ€ãƒ¼ã®ã‚¹ã‚¿ã‚¤ãƒªãƒ³ã‚° */
    .stDataFrame th {
        background-color: #f0f0f0 !important;
        font-weight: bold !important;
        border: 3px solid #333 !important;
    }
    
    /* æ¤œç´¢ç¯„å›²åˆ—ã®ã‚¹ã‚¿ã‚¤ãƒªãƒ³ã‚° */
    div[data-testid="column"] div[data-testid="stDataFrame"] td:last-child {
        font-weight: bold;
        text-align: center;
    }
    </style>
    """, unsafe_allow_html=True)
    
    # ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã®åˆæœŸåŒ–
    if 'search_results' not in st.session_state:
        st.session_state.search_results = pd.DataFrame()
    
    # CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ï¼ˆå±•é–‹å¯èƒ½ï¼‰
    with st.expander("ğŸ“ CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰"):
        st.info("ğŸ’¡ é§…ãƒ‡ãƒ¼ã‚¿.jpï¼ˆekidata.jpï¼‰ã®ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã—ã¦ã„ã¾ã™")
        uploaded_file = st.file_uploader(
            "æœ€æ–°ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã™ã‚‹å ´åˆã¯é¸æŠã—ã¦ãã ã•ã„", 
            type=['csv'],
            help="çœç•¥ã—ãŸå ´åˆã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®é§…ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã—ã¾ã™"
        )
    
    # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    if use_fast_search and uploaded_file is None:
        # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½¿ç”¨æ™‚ã¯äº‹å‰å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨
        df = indexed_df.copy()
        # å¿…è¦ãªåˆ—ãŒä¸è¶³ã—ã¦ã„ã‚‹å ´åˆã¯è£œå®Œ
        if 'prefecture' not in df.columns:
            df['prefecture'] = df['pref_cd'].map(PREFECTURE_CODE_TO_NAME)
        if 'route_name' not in df.columns:
            df['route_name'] = "ä¸æ˜"
        if 'operator_name' not in df.columns:
            df['operator_name'] = "ä¸æ˜"
    else:
        # é€šå¸¸ã®ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
        if uploaded_file is not None:
            df = load_station_data(uploaded_file)
        else:
            df = load_station_data()
    
    if df.empty:
        st.warning("ãƒ‡ãƒ¼ã‚¿ãŒèª­ã¿è¾¼ã¾ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
        return
    
    st.success(f"ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº†: {len(df):,} ä»¶ã®é§…ãƒ‡ãƒ¼ã‚¿")
    
    # æ¤œç´¢æ–‡å­—åˆ—å…¥åŠ›
    st.subheader("1. æ¤œç´¢æ–‡å­—åˆ—å…¥åŠ›")
    search_input = st.text_input(
        "æ¤œç´¢æ–‡å­—åˆ—ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ï¼ˆã²ã‚‰ãŒãªãƒ»æ¼¢å­—å¯¾å¿œã€æœ€å¤§20æ–‡å­—ï¼‰",
        max_chars=20,
        help="è¤‡æ•°é§…åã§ç¸¦ã‚¯ãƒ­ã‚¹ãƒ¯ãƒ¼ãƒ‰ã‚’æ¤œç´¢ï¼‰"
    )
    
    # ã‚«ã‚¿ã‚«ãƒŠå‡¦ç†ã®åˆ‡ã‚Šæ›¿ãˆ
    include_katakana = st.checkbox(
        "ã‚«ã‚¿ã‚«ãƒŠã‚’åˆ¥æ–‡å­—ã¨ã—ã¦æ‰±ã†",
        value=False,
        help="ã‚ªãƒ³: ã‚«ã‚¿ã‚«ãƒŠã‚’ãã®ã¾ã¾æ¤œç´¢å¯¾è±¡ã¨ã™ã‚‹ï¼ˆã€Œã‚ã€ã¨ã€Œã‚¢ã€ã‚’åˆ¥æ–‡å­—æ‰±ã„ï¼‰\nã‚ªãƒ•: ã‚«ã‚¿ã‚«ãƒŠã‚’ã²ã‚‰ãŒãªã«å¤‰æ›ã—ã¦æ¤œç´¢ï¼ˆã€Œã‚ã€ã¨ã€Œã‚¢ã€ã‚’åŒã˜æ–‡å­—æ‰±ã„ï¼‰"
    )

    
    # éƒ½é“åºœçœŒãƒ»åœ°æ–¹é¸æŠ
    st.subheader("2. åœ°æ–¹ãƒ»éƒ½é“åºœçœŒé¸æŠ")
    
    # åœ°æ–¹åŒºåˆ†ã¨éƒ½é“åºœçœŒã®ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã‚’å–å¾—
    prefecture_options = get_prefecture_options()
    
    selected_options = st.multiselect(
        "æ¤œç´¢å¯¾è±¡ã¨ã™ã‚‹åœ°æ–¹ãƒ»éƒ½é“åºœçœŒã‚’é¸æŠã—ã¦ãã ã•ã„ï¼ˆè¤‡æ•°é¸æŠå¯ã€æœªé¸æŠæ™‚ã¯å…¨å›½å¯¾è±¡ï¼‰",
        options=prefecture_options,
        default=[],
        help="ã€åœ°æ–¹åã€‘ã‚’é¸æŠã™ã‚‹ã¨è©²å½“åœ°æ–¹ã®å…¨éƒ½é“åºœçœŒãŒå¯¾è±¡ã«ãªã‚Šã¾ã™ã€‚\nå€‹åˆ¥ã®éƒ½é“åºœçœŒã‚‚é¸æŠå¯èƒ½ã§ã™ã€‚"
    )
    
    # é¸æŠã•ã‚ŒãŸã‚ªãƒ—ã‚·ãƒ§ãƒ³ã‹ã‚‰éƒ½é“åºœçœŒã‚³ãƒ¼ãƒ‰ã‚’å–å¾—
    selected_prefecture_codes = get_selected_prefecture_codes(selected_options)
    
    if selected_options:
        selected_pref_names = [PREFECTURE_CODE_TO_NAME[code] for code in selected_prefecture_codes if code in PREFECTURE_CODE_TO_NAME]
        st.info(f"é¸æŠä¸­ã®éƒ½é“åºœçœŒ: {', '.join(selected_pref_names)}")
    
    # æ¤œç´¢å®Ÿè¡Œ
    if st.button("æ¤œç´¢å®Ÿè¡Œ", type="primary") or search_input or selected_options != st.session_state.get('prev_selected', []):
        st.session_state.prev_selected = selected_options
        
        with st.spinner('æ¤œç´¢ä¸­...'):
            # é«˜é€Ÿæ¤œç´¢ã‹ã©ã†ã‹ã§å‡¦ç†ã‚’åˆ†å²
            if use_fast_search and uploaded_file is None:
                results = search_and_analyze_fast(df, search_input, selected_prefecture_codes, hiragana_index, katakana_index, include_katakana)
            else:
                results = search_and_analyze(df, search_input, selected_prefecture_codes, include_katakana)
            st.session_state.search_results = results
    
    # æ¤œç´¢çµæœè¡¨ç¤º
    results = st.session_state.search_results
    
    st.subheader("3. æ¤œç´¢çµæœ")
    
    if not results.empty:
        st.success(f"ã‚¯ãƒ­ã‚¹ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢å¯èƒ½: {len(results):,} é§…ã§ãƒ‘ã‚¿ãƒ¼ãƒ³ç™ºè¦‹")
        
        # é¸æŠä¸­ã®éƒ½é“åºœçœŒã‚’è¡¨ç¤ºï¼ˆæ—¢ã«ä¸Šéƒ¨ã§è¡¨ç¤ºæ¸ˆã¿ã®ãŸã‚å‰Šé™¤ï¼‰
        
        # æ¤œç´¢æ–‡å­—åˆ—ã‚’è¡¨ç¤º
        if search_input:
            normalized = normalize_search_string(search_input, include_katakana)
            if normalized:
                katakana_mode = "ã‚«ã‚¿ã‚«ãƒŠåˆ¥æ–‡å­—æ‰±ã„" if include_katakana else "ã‚«ã‚¿ã‚«ãƒŠâ†’ã²ã‚‰ãŒãªå¤‰æ›"
                st.info(f"æ¤œç´¢æ–‡å­—åˆ—: {normalized} ï¼ˆ{katakana_mode}ï¼‰")
        
        # çµæœãƒ†ãƒ¼ãƒ–ãƒ«è¡¨ç¤ºï¼ˆä½ç½®åˆ¥ã«ã‚°ãƒ«ãƒ¼ãƒ—åŒ–ï¼‰
        if 'char_position' in results.columns:
            # å‡¡ä¾‹è¡¨ç¤º
            st.markdown("""
            **ğŸ“‹ è‰²åˆ†ã‘å‡¡ä¾‹**
            - <span style="background-color: #e3f2fd; padding: 2px 8px; border-radius: 3px;">ğŸ”µ é¸æŠåœ°åŸŸå†…</span>
            - <span style="background-color: #fff3e0; padding: 2px 8px; border-radius: 3px;">ğŸ”´ å…¨å›½</span>
            
            """, unsafe_allow_html=True)
            
            positions = sorted(results['char_position'].unique())
            
            for pos in positions:
                st.subheader(f"ğŸ“ {pos}æ–‡å­—ç›®ã§ã®çµ„ã¿åˆã‚ã›")
                pos_results = results[results['char_position'] == pos]
                
                st.dataframe(
                    style_dataframe(pos_results),
                    use_container_width=True,
                    hide_index=True,
                    column_config={
                        "station_name": "é§…å",
                        "prefecture": "éƒ½é“åºœçœŒ",
                        "operator_name": "äº‹æ¥­è€…",
                        "route_name": "è·¯ç·šå",
                        "search_char": "å¯¾å¿œæ–‡å­—",
                        "char_position": st.column_config.NumberColumn("æ–‡å­—ä½ç½®", format="%d"),
                        "search_scope": st.column_config.TextColumn(
                            "æ¤œç´¢ç¯„å›²",
                            help="ğŸ”µ é¸æŠåœ°åŸŸå†… / ğŸ”´ å…¨å›½"
                        )
                    }
                )
                
                # é¸æŠåœ°åŸŸå†…ã®é§…æ•°ã‚’è¡¨ç¤º
                selected_count = len(pos_results[pos_results['search_scope'] == 'ğŸ”µ é¸æŠåœ°åŸŸå†…'])
                total_count = len(pos_results)
                if selected_count > 0:
                    st.success(f"ğŸ”µ é¸æŠåœ°åŸŸå†…: {selected_count}é§… / å…¨ä½“: {total_count}é§…")
        else:
            # ä½ç½®æƒ…å ±ãŒãªã„å ´åˆã¯å¾“æ¥é€šã‚Šè¡¨ç¤º
            st.dataframe(
                style_dataframe(results),
                use_container_width=True,
                hide_index=True,
                column_config={
                    "station_name": "é§…å",
                    "prefecture": "éƒ½é“åºœçœŒ",
                    "operator_name": "äº‹æ¥­è€…",
                    "route_name": "è·¯ç·šå",
                    "search_char": "å¯¾å¿œæ–‡å­—",
                    "char_position": st.column_config.NumberColumn("æ–‡å­—ä½ç½®", format="%d"),
                    "search_scope": st.column_config.TextColumn(
                        "æ¤œç´¢ç¯„å›²",
                        help="ğŸ”µ é¸æŠåœ°åŸŸå†… / ğŸ”´ å…¨å›½"
                    )
                }
            )
        
        # CSVãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
        st.subheader("4. CSVãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰")
        csv_data = create_download_csv(results)
        
        if csv_data:
            st.download_button(
                label="æ¤œç´¢çµæœã‚’CSVã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                data=csv_data,
                file_name=f"station_search_results_{len(results)}ä»¶.csv",
                mime="text/csv",
                type="secondary"
            )
    else:
        if search_input or selected_options:
            st.warning("æŒ‡å®šæ¡ä»¶ã§ç¸¦ã‚¯ãƒ­ã‚¹ãƒ¯ãƒ¼ãƒ‰ãŒæ¤œç´¢ã§ãã¾ã›ã‚“")
            st.info("ğŸ’¡ ãƒ’ãƒ³ãƒˆ: æ¤œç´¢æ–‡å­—åˆ—ã®å„æ–‡å­—ãŒåŒã˜ä½ç½®ï¼ˆä½•æ–‡å­—ç›®ï¼‰ã«ã‚ã‚‹é§…ã®çµ„ã¿åˆã‚ã›ãŒå¿…è¦ã§ã™")
        else:
            st.info("æ¤œç´¢æ–‡å­—åˆ—ã‚’å…¥åŠ›ã—ã¦ç¸¦ã‚¯ãƒ­ã‚¹ãƒ¯ãƒ¼ãƒ‰ã‚’æ¤œç´¢ã—ã¦ãã ã•ã„")


if __name__ == "__main__":
    main()
